# üå≥ Decision Tree: T·ª´ C∆° b·∫£n ƒë·∫øn N√¢ng cao

Decision Tree (C√¢y quy·∫øt ƒë·ªãnh) l√† m·ªôt trong nh·ªØng thu·∫≠t to√°n **Supervised Learning** ph·ªï bi·∫øn nh·∫•t trong Machine Learning. N√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c·∫£ **Classification (Ph√¢n lo·∫°i)** v√† **Regression (H·ªìi quy)**.

---

## üìå 1. Decision Tree l√† g√¨?

Decision Tree l√† m·ªôt c·∫•u tr√∫c ph√¢n c·∫•p t∆∞∆°ng t·ª± nh∆∞ s∆° ƒë·ªì lu·ªìng (flowchart):

- **Root Node (N√∫t g·ªëc):** ƒê·∫°i di·ªán cho to√†n b·ªô t·∫≠p d·ªØ li·ªáu.
- **Internal Node (N√∫t ƒëi·ªÅu ki·ªán):** ƒê·∫°i di·ªán cho m·ªôt thu·ªôc t√≠nh (feature) v√† m·ªôt c√¢u h·ªèi quy·∫øt ƒë·ªãnh.
- **Leaf Node (N√∫t l√°):** ƒê·∫°i di·ªán cho k·∫øt qu·∫£ cu·ªëi c√πng (nh√£n l·ªõp ho·∫∑c gi√° tr·ªã s·ªë).

---

## ‚öñÔ∏è 2. Ph√¢n bi·ªát Classification v√† Regression Tree

| ƒê·∫∑c ƒëi·ªÉm | Classification Tree | Regression Tree |
|----------|---------------------|-----------------|
| **M·ª•c ti√™u** | D·ª± ƒëo√°n nh√£n l·ªõp (v√≠ d·ª•: Spam/Not Spam) | D·ª± ƒëo√°n gi√° tr·ªã li√™n t·ª•c (v√≠ d·ª•: Gi√° nh√†) |
| **Gi√° tr·ªã n√∫t l√°** | Nh√£n l·ªõp xu·∫•t hi·ªán nhi·ªÅu nh·∫•t (Mode) | Gi√° tr·ªã trung b√¨nh c·ªßa c√°c m·∫´u (Mean) |
| **Ti√™u ch√≠ chia n√∫t** | Gini Impurity ho·∫∑c Entropy (Information Gain) | Variance Reduction ho·∫∑c Mean Squared Error (MSE) |

---

## üõ†Ô∏è 3. C√°c thu·∫≠t to√°n chia n√∫t ph·ªï bi·∫øn

### üîπ Cho Classification
- **Gini Impurity**  
  

\[
  Gini = 1 - \sum_{i=1}^{n} (P_i)^2
  \]

  
  ‚Üí ƒêo m·ª©c ƒë·ªô "v·∫©n ƒë·ª•c" c·ªßa d·ªØ li·ªáu. C√†ng g·∫ßn 0 th√¨ d·ªØ li·ªáu c√†ng thu·∫ßn khi·∫øt.

- **Entropy (Information Gain)**  
  

\[
  Entropy = - \sum_{i=1}^{n} P_i \cdot \log_2(P_i)
  \]

  
  ‚Üí ƒêo ƒë·ªô h·ªón lo·∫°n th√¥ng tin.

### üîπ Cho Regression
- **Mean Squared Error (MSE)**  
  

\[
  MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
  \]

  
  ‚Üí Chia sao cho t·ªïng b√¨nh ph∆∞∆°ng sai l·ªách gi·ªØa gi√° tr·ªã th·ª±c v√† gi√° tr·ªã trung b√¨nh t·∫°i c√°c n√∫t con l√† nh·ªè nh·∫•t.

---

## üíª 4. Code minh h·ªça (Python & Scikit-learn)

```python
import pandas as pd
from sklearn.datasets import load_iris, fetch_california_housing
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_text
from sklearn.model_selection import train_test_split

# --- 1. CLASSIFICATION (Ph√¢n lo·∫°i hoa Iris) ---
iris = load_iris()
X_clf, y_clf = iris.data, iris.target

clf_tree = DecisionTreeClassifier(max_depth=3, criterion='gini')
clf_tree.fit(X_clf, y_clf)

print("--- C·∫•u tr√∫c c√¢y ph√¢n lo·∫°i ---")
print(export_text(clf_tree, feature_names=iris.feature_names))

# --- 2. REGRESSION (D·ª± b√°o gi√° nh√† California) ---
housing = fetch_california_housing()
X_reg, y_reg = housing.data[:500], housing.target[:500]  # L·∫•y m·∫´u nh·ªè ƒë·ªÉ demo

reg_tree = DecisionTreeRegressor(max_depth=3)
reg_tree.fit(X_reg, y_reg)

print("\n--- C·∫•u tr√∫c c√¢y h·ªìi quy ---")
print(export_text(reg_tree, feature_names=housing.feature_names))
```

---

## üéØ 5. Khi n√†o ch·ªçn ch·ªâ s·ªë n√†o? (Cheat Sheet)

### Cho Classification
| T√¨nh hu·ªëng | Ch·ªâ s·ªë ∆∞u ti√™n |
|------------|----------------|
| D·ªØ li·ªáu c√¢n b·∫±ng | Accuracy |
| Mu·ªën tr√°nh b√°o ƒë·ªông gi·∫£ | Precision |
| Mu·ªën tr√°nh b·ªè s√≥t b·ªánh nh√¢n/t·ªôi ph·∫°m | Recall |
| D·ªØ li·ªáu m·∫•t c√¢n b·∫±ng | F1-Score / AUC-ROC |

### Cho Regression
| T√¨nh hu·ªëng | Ch·ªâ s·ªë ∆∞u ti√™n | L√Ω do |
|------------|----------------|-------|
| D·ªØ li·ªáu c√≥ nhi·ªÅu Outliers | MAE | Kh√¥ng b·ªã l·ªói l·ªõn l√†m sai l·ªách k·∫øt qu·∫£ |
| Mu·ªën ph·∫°t n·∫∑ng c√°c l·ªói l·ªõn | RMSE / MSE | L·ªói c√†ng l·ªõn th√¨ "h√¨nh ph·∫°t" c√†ng n·∫∑ng |
| B√°o c√°o kinh doanh | MAPE | D·ªÖ hi·ªÉu d∆∞·ªõi d·∫°ng ph·∫ßn trƒÉm sai s·ªë |
| ƒê√°nh gi√° ƒë·ªô kh·ªõp t·ªïng qu√°t | R-squared | Bi·∫øt m√¥ h√¨nh t·ªët h∆°n ƒëo√°n m√≤ bao nhi√™u ph·∫ßn trƒÉm |

---

## ‚ö†Ô∏è 6. ∆Øu v√† Nh∆∞·ª£c ƒëi·ªÉm

**∆Øu ƒëi·ªÉm:**
- D·ªÖ hi·ªÉu, d·ªÖ tr·ª±c quan h√≥a.
- Kh√¥ng c·∫ßn chu·∫©n h√≥a d·ªØ li·ªáu (scaling).
- X·ª≠ l√Ω ƒë∆∞·ª£c c·∫£ d·ªØ li·ªáu s·ªë v√† ph√¢n lo·∫°i.

**Nh∆∞·ª£c ƒëi·ªÉm:**
- D·ªÖ b·ªã **Overfitting** (qu√° kh·ªõp).
- Nh·∫°y c·∫£m v·ªõi d·ªØ li·ªáu nhi·ªÖu.
- C·∫ßn gi·ªõi h·∫°n `max_depth` ho·∫∑c s·ª≠ d·ª•ng **Random Forest** ƒë·ªÉ kh·∫Øc ph·ª•c.

---

## üìä 7. Visualization (Tu·ª≥ ch·ªçn)

B·∫°n c√≥ th·ªÉ v·∫Ω c√¢y quy·∫øt ƒë·ªãnh b·∫±ng th∆∞ vi·ªán **Graphviz** ho·∫∑c **Matplotlib** ƒë·ªÉ tr·ª±c quan h√≥a m√¥ h√¨nh. V√≠ d·ª• v·ªõi Graphviz:

```python
from sklearn import tree
import graphviz

dot_data = tree.export_graphviz(clf_tree, out_file=None,
                                feature_names=iris.feature_names,
                                class_names=iris.target_names,
                                filled=True, rounded=True,
                                special_characters=True)

graph = graphviz.Source(dot_data)
graph.render("decision_tree")
